{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read all the files and saved in fileSeries Dataframe\n",
    "\n",
    "def readFilesFromDirectory(directoryName):     \n",
    "    fileSeries = pd.Series(data=[], index=[])\n",
    "    \n",
    "    with os.scandir(directoryName) as entries:\n",
    "        for entry in entries:\n",
    "            f = open(directoryName + \"/\" + entry.name, \"rb\")\n",
    "            content = f.read().decode('ISO-8859-1').replace(\"\\n\", \" \")\n",
    "            fileSeries[entry.name] = content.replace(\"\\r\",\"\")\n",
    "            f.close()\n",
    "    return fileSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 31.85650610923767\n",
      "Total files read: 21941\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Will be using fileSeries dataframe to access document and its text\n",
    "fileSeries = readFilesFromDirectory(\"./ACL txt\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Execution Time:\", end - start) \n",
    "print(\"Total files read:\", len(fileSeries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunc(doc):\n",
    "    stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = nltk.WordPunctTokenizer().tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and  len(token) > 2 and token.isalpha()]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(wordList):\n",
    "    return {word:wordList.count(word) for word in set(wordList)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 786.435583114624\n"
     ]
    }
   ],
   "source": [
    "#created termFrequencyDocuments Dictionary for each document \n",
    "\n",
    "#removed stopword and punctuations then calculated term freqency for each document and created dictionary of dictionaries\n",
    "\n",
    "# termFrequencyDocuments dictionary will be used for rest of the tasks\n",
    "start = time.time()\n",
    "termFrequencyDocuments = {}\n",
    "\n",
    "for index, value in fileSeries.items():\n",
    "    processedDoc = removePunc(value)\n",
    "    termFrequencyDocuments[index]=termFrequencyInDoc(processedDoc)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(\"Execution Time:\", end - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#each word document frequency calculated\n",
    "\n",
    "def wordDocFre(termFreqDictionary): \n",
    "    documentFrequencies = {}\n",
    "    \n",
    "    for docId, docValue in termFreqDictionary.items():\n",
    "        for word, value in docValue.items():\n",
    "            try:\n",
    "                documentFrequencies[word]+=1\n",
    "            except:\n",
    "                documentFrequencies[word] = 1\n",
    "\n",
    "    return documentFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documentFrequencies = wordDocFre(termFrequencyDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverseDocFre(wordDocFreqDictionary): \n",
    "    M = len(termFrequencyDocuments)\n",
    "    return {key: np.log2((M+1)/value)  for key, value in wordDocFreqDictionary.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs= inverseDocFre(documentFrequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yas\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set([key for key in documentFrequencies.keys()]))\n",
    "if(\"lda\" in  list(documentFrequencies.keys())):\n",
    "    print(\"yas\")\n",
    "#Created dictionay to saved each vocabulary word index in dictionary so i can populate document and query vectors with idf and freq\n",
    "\n",
    "vocabIndexDictionary = { value: index for index,value in enumerate(vocabulary) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfIdf(termFreq, inverseDocFreq):\n",
    "    \n",
    "    docTfIdfVector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for token in termFreq.keys():\n",
    "        \n",
    "        tfidf  = termFreq[token] * inverseDocFreq[token]\n",
    "        \n",
    "        index = vocabIndexDictionary[token]\n",
    "        \n",
    "        docTfIdfVector[index] = tfidf\n",
    "        \n",
    "    return docTfIdfVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creater frequency vector for query \n",
    "def getQueryVector(query):\n",
    "    queryFreqVector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    queryTermFreq = termFrequencyInDoc(removePunc(query))\n",
    "    \n",
    "    for token, freq in queryTermFreq.items():\n",
    "        \n",
    "        index = vocabIndexDictionary[token]\n",
    "        queryFreqVector[index] = freq\n",
    "        \n",
    "    return queryFreqVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VSM(query):\n",
    "    #created term frequency of query\n",
    "    \n",
    "    queryVector = getQueryVector(query)\n",
    "    \n",
    "    docSimilarityScore = {}\n",
    "    \n",
    "    for docId, docTermFreq in termFrequencyDocuments.items():\n",
    "        \n",
    "        docVector = tfIdf(docTermFreq, idfs) #per doc tfId calculated\n",
    "        docSimilarityScore[docId] = np.dot(queryVector, docVector, out=None)  \n",
    "    \n",
    "    return docSimilarityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def sortByScore(scoreDict):\n",
    "    return sorted(scoreDict.items(), key=operator.itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('D11-1050.pdf.txt', 752.3030201019424), ('D13-1172.pdf.txt', 676.1204357878216), ('D09-1026.pdf.txt', 628.5063205914962), ('Q15-1022.pdf.txt', 538.0395017184778), ('W11-2506.pdf.txt', 538.0395017184778)]\n"
     ]
    }
   ],
   "source": [
    "query1Result = VSM(\"LDA\")\n",
    "\n",
    "sortedByRank = sortByScore(query1Result)\n",
    "\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('P12-1079.pdf.txt', 450.811570284626), ('J14-2003.pdf.txt', 406.44035273692657), ('N15-1074.pdf.txt', 315.9230689396198), ('Q15-1004.pdf.txt', 312.37337153580387), ('Q15-1022.pdf.txt', 289.30033841100015)]\n"
     ]
    }
   ],
   "source": [
    "query2Result = VSM(\"Topic modelling\")\n",
    "\n",
    "sortedByRank = sortByScore(query2Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W06-1668.pdf.txt', 187.19916366688074), ('W11-0100.pdf.txt', 182.22356277111322), ('J03-4003.pdf.txt', 151.16826455117942), ('D09-1111.pdf.txt', 135.30130968802501), ('D09-1058.pdf.txt', 131.9145646377607)]\n"
     ]
    }
   ],
   "source": [
    "query3Result = VSM(\"Generative models\")\n",
    "\n",
    "sortedByRank = sortByScore(query3Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W11-0100.pdf.txt', 1000.0019722646931), ('J08-2004.pdf.txt', 196.68702411982164), ('W15-3808.pdf.txt', 182.405180869523), ('W04-1801.pdf.txt', 171.76074940901188), ('J09-2003.pdf.txt', 161.91553812211853)]\n"
     ]
    }
   ],
   "source": [
    "query4Result = VSM(\"Semantic relationships between terms\")\n",
    "\n",
    "sortedByRank = sortByScore(query4Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W11-0100.pdf.txt', 155.38103787162342), ('J14-1005.pdf.txt', 88.99281941873132), ('J87-1020.pdf.txt', 87.51531375973897), ('W14-55.x.pdf.txt', 60.85516369287274), ('J87-3010.pdf.txt', 53.78284302032024)]\n"
     ]
    }
   ],
   "source": [
    "query5Result = VSM(\"Natural Language Processing\")\n",
    "\n",
    "sortedByRank = sortByScore(query5Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('P06-1062.pdf.txt', 161.61688577674002), ('D09-1162.pdf.txt', 161.36684200961096), ('P12-1062.pdf.txt', 122.68305517578517), ('W09-2609.pdf.txt', 119.45461191336705), ('W11-0100.pdf.txt', 114.51126888597759)]\n"
     ]
    }
   ],
   "source": [
    "query6Result = VSM(\"Text Mining\")\n",
    "\n",
    "sortedByRank = sortByScore(query6Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J85-2006.pdf.txt', 447.89495301256284), ('J03-3003.pdf.txt', 421.06976616764905), ('J06-4004.pdf.txt', 352.2254777771003), ('J03-3004.pdf.txt', 329.58272945986806), ('W14-3302.pdf.txt', 327.6765620156608)]\n"
     ]
    }
   ],
   "source": [
    "query7Result = VSM(\"Translation model\")\n",
    "\n",
    "sortedByRank = sortByScore(query7Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W11-0100.pdf.txt', 242.5921091987321), ('J87-3007.pdf.txt', 211.77949355101745), ('W06-1647.pdf.txt', 197.3033530148868), ('W10-2505.pdf.txt', 196.50598280569395), ('W99-0630.pdf.txt', 190.92134755129433)]\n"
     ]
    }
   ],
   "source": [
    "query8Result = VSM(\"Learning procedures for the lexicon\")\n",
    "\n",
    "sortedByRank = sortByScore(query8Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W11-0100.pdf.txt', 867.6438855111824), ('J09-4008.pdf.txt', 196.78731141091293), ('J08-2004.pdf.txt', 195.88572195636618), ('J09-2003.pdf.txt', 158.7103294682967), ('J14-1002.pdf.txt', 135.32721096794992)]\n"
     ]
    }
   ],
   "source": [
    "query9Result = VSM(\"Semantic evaluations\")\n",
    "\n",
    "sortedByRank = sortByScore(query9Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W11-0100.pdf.txt', 218.2504424692338), ('N10-1141.pdf.txt', 154.05405347061145), ('J01-2002.pdf.txt', 153.8651240339329), ('P11-1127.pdf.txt', 131.17937259694367), ('J11-3003.pdf.txt', 129.13058664334983)]\n"
     ]
    }
   ],
   "source": [
    "query10Result = VSM(\"System results and combination\")\n",
    "\n",
    "sortedByRank = sortByScore(query10Result)\n",
    "\n",
    "print(sortedByRank[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
